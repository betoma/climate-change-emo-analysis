# climate-change-emo-analysis

A project for Dr. Stede's Sentiment &amp; Argument Mining PM

The data consists of tokenized tweets concerning climate change scraped from Twitter. 1999 have been classified into categories based on whether participants in our crowd study believed that the person who wrote that tweet believed that anthropogenic climate change is a serious global issue or whether they either disbelieved in climate change or believed it was not man-made. Each tweet received at least three annotations and was sorted as either a "believer", a "denier", or an "uncertain". The tweets and labels themselves are not included in this repository to comply with Twitter's terms of use.

SVMs were trained as classifiers on this annotated data. Each SVMs hyperparameters were optimized using grid search. Several different variants were trained -- one on word-level unigrams, one on word-level unigrams and bigrams, one on character-level 4-grams, and one on characer-level 5-grams. All performed quite poorly, with F1s of around 0.5, with the character-level 4- and 5-grams performing best by a small amount. Another SVM was trained on vectors created from discourse features using the code in `discourseparse.py`, an emulation of the lightweight discourse analysis algorithm from [Mukherjee & Bhattacharyya 2012](https://www.cse.iitb.ac.in/~pb/papers/coling12-discourse-sa.pdf). This model performed somewhat worse than the other models, with an F1 of around 0.48. `svm-classification.ipynb` contains this code as well as the evaluation of the performance of these SVM classifiers showing the exact accuracy, precision, recall, and F1 values for each classifier model.

Comparison of these models to fine-tuned BERT was originally planned, but due to unforeseen hardware issues was not completed. The jupyter notebooks written for this task are included regardless as `fine-tuning.ipynb` (intended to fine-tune the language-masking model on the ~900,000 unlabelled climate change tweets) and `bert-classification.ipynb` (intended to further fine-tune this model on the classification task).

Lexical emotion analysis was performed using data from the [NRC Sentiment & Emotion Lexicons](http://saifmohammad.com/WebPages/lexicons.html), created by Saif M. Mohammad and Peter D. Turney at the National Research Council Canada -- in particular the manually-construction NRC Emotion Lexicon, NRC Affect Intensity Lexicon, and NRC Valence-Arousal-Dominance Lexicon. These lexicons are not included in this repository due to their terms of use. If you're interested in replicating this experiment, you can find the lexicons used [here](http://saifmohammad.com/WebPages/AccessResource.htm). This emotion analysis was performed using the bag-of-words method alone, simply finding the mean of the values for each word present in the lexicon within the tweet. `emonal.py` includes the code used to perform this analysis.

When averaged across all the tweets in each category, most of the differences were small. Among these differences, the largest differences (>0.02) include a higher number and intensity of "anger" lexical items in "denier" tweets than in both "believer" and "uncertain" tweets, higher number of "disgust" lexical items in "denier" tweets than in both "believer" and "uncertain" tweets, higher valence of "denier" tweets than "uncertain" tweets (but not "believer" tweets), and higher intensity of "joy" lexical items of "denier" tweets than "uncertain" tweets (but not "believer" tweets). These numbers are listed in more detail within `emotions.ipynb`.
