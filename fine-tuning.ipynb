{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from transformers import WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with Unlabelled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') #or cpu\n",
    "MODEL_NAME = \"bert-base-cased\" #or one of the other pretrained models listed at https://huggingface.co/transformers/pretrained_models.html\n",
    "\n",
    "TRAIN_DATA_FILE = \"D:/Users/Beth/Documents/tweet_data/cc_tweet_data.txt\"\n",
    "OUTPUT_DIR = \"D:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/\"\n",
    "\n",
    "MAX_LENGTH = int(64)\n",
    "RANDOM_SEED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PARAMS = {\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 1e-5,\n",
    "    'weight_decay': 1e-5,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'grad_accum_steps': 1,\n",
    "    'warmup_steps': 500,\n",
    "    'checkpoint_steps': 2500,\n",
    "    'checkpoint_dir': OUTPUT_DIR,\n",
    "    'eval_steps': 250,\n",
    "    'num_train_epochs': 1,\n",
    "    'max_steps': -1, # if >0, overrides num_train_epochs\n",
    "    'checkpoint': MODEL_NAME if MODEL_NAME.endswith('.pt') else None,\n",
    "    'mlm_probability': 0.15\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG = BertConfig.from_pretrained(MODEL_NAME)\n",
    "TOKENIZER = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case = False, config = CONFIG)\n",
    "MODEL = BertForMaskedLM.from_pretrained(MODEL_NAME, config = CONFIG)\n",
    "\n",
    "#deactivate dropout for reproducible results in evaluation, comment out otherwise\n",
    "MODEL.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in the model=108931396\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"number of parameters in the model={count_parameters(MODEL)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LineByLineDataset(Dataset):\n",
    "    def __init__(self, file_path: str, tokenizer=TOKENIZER, block_size=MAX_LENGTH):\n",
    "        assert os.path.isfile(file_path)\n",
    "        print(f\"Creating features from dataset file at {file_path}\")\n",
    "        \n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
    "        \n",
    "        self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return torch.tensor(self.examples[i], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(inputs: torch.Tensor, tokenizer=TOKENIZER, mlm_probability=0.15) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\" Prepare masked token inputs/labels for language modelling: 80% MASK, 10% random, 10% original \"\"\"\n",
    "    labels = inputs.clone()\n",
    "    probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    if tokenizer._pad_token is not None:\n",
    "        padding_mask = labels.eq(tokenizer.pad_token_id)\n",
    "        probability_matrix.masked_fill_(padding_mask, value = 0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100\n",
    "    \n",
    "    #80% of the time, we replaced masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "    \n",
    "    #10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "    \n",
    "    #the rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset, model, tokenizer, batch_size, key, mlm_probability=0.15, max_steps=None, device=torch.device('cuda')) -> Dict:\n",
    "    \n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    sampler = SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, sampler=sampler, batch_size=batch_size, collate_fn=collate\n",
    "    )\n",
    "    \n",
    "    print(f\"Beginning evaluation on dataset {key}\")\n",
    "    \n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\", leave = False,):\n",
    "        inputs, labels = mask_tokens(batch, tokenizer, mlm_probability)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, masked_lm_labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "    \n",
    "    print(f\"perplexity: {perplexity}\")\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model=MODEL, tokenizer=TOKENIZER, params = TRAIN_PARAMS, val_dataset = None, device=torch.device(\"cuda\")):\n",
    "    \n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    \n",
    "    sampler = RandomSampler(dataset)\n",
    "    dataloader = DataLoader(dataset,\n",
    "        sampler = sampler,\n",
    "        batch_size = params['batch_size'],\n",
    "        collate_fn = collate\n",
    "    )\n",
    "    \n",
    "    if params['max_steps'] > 0:\n",
    "        t_total = params['max_steps']\n",
    "        num_epochs = params['max_steps'] // len(dataloader) // params['grad_accum_steps'] + 1\n",
    "    else:\n",
    "        t_total = len(dataloader) // params['grad_accum_steps'] * params['num_train_epochs']\n",
    "        num_epochs = params['num_train_epochs']\n",
    "    \n",
    "    #prepare optimizer and scheduler (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_params = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": params[\"weight_decay\"]},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_params, lr=params[\"learning_rate\"], eps=params[\"adam_epsilon\"])\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=params[\"warmup_steps\"], num_training_steps=t_total)\n",
    "    print(\"training...\")\n",
    "    print(f\"num examples:\\t\\t\\t{len(dataset)}\")\n",
    "    print(f\"num epochs:\\t\\t\\t{num_epochs}\")\n",
    "    if params[\"grad_accum_steps\"] > 1:\n",
    "        print(\"gradient accumulation steps:\\t{}\".format(params[\"grad_accum_steps\"]))\n",
    "        print(\"batch size with accumulation:\\t{}\".format(params[\"batch_size\"]))\n",
    "    else:\n",
    "        print(\"batch size:\\t\\t\\t{}\".format(params[\"batch_size\"]))\n",
    "    print(f\"total optimization steps:\\t{t_total}\")\n",
    "    \n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    if params['checkpoint']:\n",
    "        opt_path = os.path.join(params['checkpoint'], 'optimizer.pt')\n",
    "        sch_path = os.path.join(params['checkpoint'], 'scheduler.pt')\n",
    "        if os.path.isfile(opt_path) and os.path.isfile(sch_path):\n",
    "            print(\"\\nupdating optimizer and scheduler from checkpoint\")\n",
    "            optimizer.load_state_dict(torch.load(opt_path))\n",
    "            scheduler.load_state_dict(torch.load(sch_path))\n",
    "        \n",
    "        try:\n",
    "            global_step = int(params['checkpoint'].split('-')[-1].split('/')[0])\n",
    "            epochs_trained = global_step // len(dataloader) // params['grad_accum_steps']\n",
    "            steps_trained_in_current_epoch = global_step % (len(dataloader) // param['grad_accum_steps'])\n",
    "            print(f\"\\npicking up from checkpoint at global step:\\t{global_step}\")\n",
    "            print(f\"continuing training from epoch:\\t\\t{epochs_trained}\")\n",
    "            print(f\"skipping first steps in epoch:\\t\\t{steps_trained_in_current_epoch}\")\n",
    "        except ValueError:\n",
    "            print(\"could not update current steps/epoch from checkpoint name\")\n",
    "    \n",
    "    training_loss, logging_loss = 0.0, 0.0\n",
    "    \n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.zero_grad()\n",
    "    \n",
    "    train_iterator = trange(epochs_trained, num_epochs, desc=\"epoch\")\n",
    "    \n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(dataloader, desc=\"iteration\")\n",
    "        \n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "            \n",
    "            inputs, labels = mask_tokens(batch, tokenizer, params['mlm_probability'])\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, masked_lm_labels=labels)\n",
    "            loss = outputs[0]\n",
    "            \n",
    "            if params['grad_accum_steps'] > 1:\n",
    "                loss = loss / params['grad_accum_steps']\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            training_loss += loss.item()\n",
    "            \n",
    "            if (step+1) % params['grad_accum_steps'] == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), params['max_grad_norm'])\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "                \n",
    "                if params['checkpoint_steps'] > 0 and global_step % params['checkpoint_steps'] == 0:\n",
    "                    save_path = os.path.join(params['checkpoint_dir'], f\"checkpoint--{global_step}\")\n",
    "                    os.makedirs(save_path, exist_ok=True)\n",
    "                    \n",
    "                    model.save_pretrained(save_path)\n",
    "                    #tokenizer.save_pretrained(save_path)\n",
    "                    \n",
    "                    print(f\"saving model checkpoint to:\\t{save_path}\")\n",
    "                    torch.save(params, os.path.join(save_path, 'training_args.bin'))\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(save_path, 'optimizer.pt'))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(save_path, 'scheduler.pt'))\n",
    "                    \n",
    "                if params['eval_steps'] > 0 and global_step % params['eval_steps'] == 0:\n",
    "                    if val_dataset:\n",
    "                        evaluate(val_dataset, model, tokenizer, params['batch_size'], params['mlm_probability'], 'val', device=device)\n",
    "                        evaluate(dataset, model, tokenizer, params['batch_size'], params['mlm_probability'], 'train', max_steps = 200, device=device)\n",
    "                    print(f\"loss:\\t\\t\\t{training_loss/global_step}\")\n",
    "                    print()\n",
    "            \n",
    "            if params['max_steps'] > 0 and global_step > params[\"max_steps\"]:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "                \n",
    "        if params['max_steps'] > 0 and global_step > params['max_steps']:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "        \n",
    "    print('saving final model to:\\t', params['checkpoint_dir'])\n",
    "    model.save_pretrained(params['checkpoint_dir'])\n",
    "    #tokenizer.save_pretrained(params['checkpoint_dir'])\n",
    "    torch.save(params, os.path.join(params['checkpoint_dir'], 'training_args.bin'))\n",
    "    \n",
    "    return global_step, training_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate perplexity of BERT on this data prior to fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at D:/Users/Beth/Documents/tweet_data/cc_tweet_data.txt\n"
     ]
    }
   ],
   "source": [
    "if 'train_dataset' not in globals():\n",
    "    train_dataset = LineByLineDataset(TRAIN_DATA_FILE)\n",
    "    #val_dataset = LineByLineDataset(VAL_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning evaluation on dataset train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=121042.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 46.09406280517578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(46.0941)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(train_dataset, MODEL, TOKENIZER, TRAIN_PARAMS['batch_size'], 'train', TRAIN_PARAMS['mlm_probability'], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune on unlabelled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "num examples:\t\t\t968331\n",
      "num epochs:\t\t\t1\n",
      "batch size:\t\t\t8\n",
      "total optimization steps:\t121042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625660ebb7184bdbad25b2dfe34db4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2a4da2027f4ee3af6d765fb781c9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='iteration', max=121042.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:\t\t\t3.606213059425354\n",
      "\n",
      "loss:\t\t\t3.339893476366997\n",
      "\n",
      "loss:\t\t\t3.2189694543679557\n",
      "\n",
      "loss:\t\t\t3.1350197797417643\n",
      "\n",
      "loss:\t\t\t3.0648388804912567\n",
      "\n",
      "loss:\t\t\t3.0124473676284156\n",
      "\n",
      "loss:\t\t\t2.981946626492909\n",
      "\n",
      "loss:\t\t\t2.93802492287755\n",
      "\n",
      "loss:\t\t\t2.9019777399169073\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--2500\n",
      "loss:\t\t\t2.867158103632927\n",
      "\n",
      "loss:\t\t\t2.8477999869476665\n",
      "\n",
      "loss:\t\t\t2.818963876903057\n",
      "\n",
      "loss:\t\t\t2.799591643388455\n",
      "\n",
      "loss:\t\t\t2.7840047210880687\n",
      "\n",
      "loss:\t\t\t2.768163042314847\n",
      "\n",
      "loss:\t\t\t2.7536722036376595\n",
      "\n",
      "loss:\t\t\t2.7389468030158213\n",
      "\n",
      "loss:\t\t\t2.7228735106852318\n",
      "\n",
      "loss:\t\t\t2.7144177089051196\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--5000\n",
      "loss:\t\t\t2.7029285488426686\n",
      "\n",
      "loss:\t\t\t2.695339107257979\n",
      "\n",
      "loss:\t\t\t2.6867705233801495\n",
      "\n",
      "loss:\t\t\t2.6801998217468674\n",
      "\n",
      "loss:\t\t\t2.6673623916258413\n",
      "\n",
      "loss:\t\t\t2.6596703480672836\n",
      "\n",
      "loss:\t\t\t2.6518531059347668\n",
      "\n",
      "loss:\t\t\t2.641180512821233\n",
      "\n",
      "loss:\t\t\t2.6363138534298964\n",
      "\n",
      "loss:\t\t\t2.6272174950293428\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--7500\n",
      "loss:\t\t\t2.621011075153947\n",
      "\n",
      "loss:\t\t\t2.6148512489555342\n",
      "\n",
      "loss:\t\t\t2.6052198325647042\n",
      "\n",
      "loss:\t\t\t2.6009071809297257\n",
      "\n",
      "loss:\t\t\t2.5948142947125086\n",
      "\n",
      "loss:\t\t\t2.5891334173432416\n",
      "\n",
      "loss:\t\t\t2.5847316241818996\n",
      "\n",
      "loss:\t\t\t2.5776504447870963\n",
      "\n",
      "loss:\t\t\t2.5720119994714072\n",
      "\n",
      "loss:\t\t\t2.5664574961257287\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--10000\n",
      "loss:\t\t\t2.561248302670568\n",
      "\n",
      "loss:\t\t\t2.5564029826848973\n",
      "\n",
      "loss:\t\t\t2.548206797885753\n",
      "\n",
      "loss:\t\t\t2.5452886983814627\n",
      "\n",
      "loss:\t\t\t2.5418407619561663\n",
      "\n",
      "loss:\t\t\t2.5374232711467477\n",
      "\n",
      "loss:\t\t\t2.5333057404542747\n",
      "\n",
      "loss:\t\t\t2.528391812957982\n",
      "\n",
      "loss:\t\t\t2.525753694056844\n",
      "\n",
      "loss:\t\t\t2.521165725826609\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--12500\n",
      "loss:\t\t\t2.517967054478526\n",
      "\n",
      "loss:\t\t\t2.5152737931170885\n",
      "\n",
      "loss:\t\t\t2.5110705581897728\n",
      "\n",
      "loss:\t\t\t2.505401222333031\n",
      "\n",
      "loss:\t\t\t2.502083684153579\n",
      "\n",
      "loss:\t\t\t2.4973343978106977\n",
      "\n",
      "loss:\t\t\t2.494012596944081\n",
      "\n",
      "loss:\t\t\t2.490389046765733\n",
      "\n",
      "loss:\t\t\t2.486426044547866\n",
      "\n",
      "loss:\t\t\t2.482717748889984\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--15000\n",
      "loss:\t\t\t2.479458297277987\n",
      "\n",
      "loss:\t\t\t2.476921753732396\n",
      "\n",
      "loss:\t\t\t2.474132058487304\n",
      "\n",
      "loss:\t\t\t2.472626649882585\n",
      "\n",
      "loss:\t\t\t2.47035514239734\n",
      "\n",
      "loss:\t\t\t2.4681451386612197\n",
      "\n",
      "loss:\t\t\t2.4663562001509196\n",
      "\n",
      "loss:\t\t\t2.463915399961952\n",
      "\n",
      "loss:\t\t\t2.4598937801381244\n",
      "\n",
      "loss:\t\t\t2.456169181298087\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--17500\n",
      "loss:\t\t\t2.4531559447284255\n",
      "\n",
      "loss:\t\t\t2.4502905110855338\n",
      "\n",
      "loss:\t\t\t2.448155312612239\n",
      "\n",
      "loss:\t\t\t2.4461201527106433\n",
      "\n",
      "loss:\t\t\t2.4440684924757963\n",
      "\n",
      "loss:\t\t\t2.4406916724407672\n",
      "\n",
      "loss:\t\t\t2.4388054045199565\n",
      "\n",
      "loss:\t\t\t2.43626721362131\n",
      "\n",
      "loss:\t\t\t2.4341620280448444\n",
      "\n",
      "loss:\t\t\t2.4331614294485955\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--20000\n",
      "loss:\t\t\t2.4301008046548813\n",
      "\n",
      "loss:\t\t\t2.42723502704354\n",
      "\n",
      "loss:\t\t\t2.42451747275053\n",
      "\n",
      "loss:\t\t\t2.42282100715192\n",
      "\n",
      "loss:\t\t\t2.4205420306445586\n",
      "\n",
      "loss:\t\t\t2.4177756555956953\n",
      "\n",
      "loss:\t\t\t2.416135240627583\n",
      "\n",
      "loss:\t\t\t2.4140564141362564\n",
      "\n",
      "loss:\t\t\t2.4128320691321385\n",
      "\n",
      "loss:\t\t\t2.409903139841021\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--22500\n",
      "loss:\t\t\t2.4076458249290784\n",
      "\n",
      "loss:\t\t\t2.405969194143683\n",
      "\n",
      "loss:\t\t\t2.4038441758842573\n",
      "\n",
      "loss:\t\t\t2.4023381441959772\n",
      "\n",
      "loss:\t\t\t2.401018514276819\n",
      "\n",
      "loss:\t\t\t2.3990834657631424\n",
      "\n",
      "loss:\t\t\t2.397868332032114\n",
      "\n",
      "loss:\t\t\t2.3962967879919663\n",
      "\n",
      "loss:\t\t\t2.393828911524646\n",
      "\n",
      "loss:\t\t\t2.3915083824408176\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--25000\n",
      "loss:\t\t\t2.3896781859517096\n",
      "\n",
      "loss:\t\t\t2.387720464657439\n",
      "\n",
      "loss:\t\t\t2.3856507688854256\n",
      "\n",
      "loss:\t\t\t2.385079927234974\n",
      "\n",
      "loss:\t\t\t2.3835555843172167\n",
      "\n",
      "loss:\t\t\t2.381907342119444\n",
      "\n",
      "loss:\t\t\t2.380354218594308\n",
      "\n",
      "loss:\t\t\t2.378079854193135\n",
      "\n",
      "loss:\t\t\t2.3770583693208516\n",
      "\n",
      "loss:\t\t\t2.37510969266104\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--27500\n",
      "loss:\t\t\t2.373125306035172\n",
      "\n",
      "loss:\t\t\t2.3714094200907527\n",
      "\n",
      "loss:\t\t\t2.3699751238695215\n",
      "\n",
      "loss:\t\t\t2.3690496356608595\n",
      "\n",
      "loss:\t\t\t2.367768144399973\n",
      "\n",
      "loss:\t\t\t2.36716606073535\n",
      "\n",
      "loss:\t\t\t2.3655533444352193\n",
      "\n",
      "loss:\t\t\t2.3638103708548424\n",
      "\n",
      "loss:\t\t\t2.363209507319887\n",
      "\n",
      "loss:\t\t\t2.3613862031157278\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--30000\n",
      "loss:\t\t\t2.360738605323434\n",
      "\n",
      "loss:\t\t\t2.3594338669727657\n",
      "\n",
      "loss:\t\t\t2.3584909293055536\n",
      "\n",
      "loss:\t\t\t2.3562504288277974\n",
      "\n",
      "loss:\t\t\t2.3546392040954482\n",
      "\n",
      "loss:\t\t\t2.3533066108751295\n",
      "\n",
      "loss:\t\t\t2.351873166455163\n",
      "\n",
      "loss:\t\t\t2.3503189050988182\n",
      "\n",
      "loss:\t\t\t2.3489620575500885\n",
      "\n",
      "loss:\t\t\t2.3477343291909425\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--32500\n",
      "loss:\t\t\t2.346303662462074\n",
      "\n",
      "loss:\t\t\t2.3449708152023208\n",
      "\n",
      "loss:\t\t\t2.3433361017989616\n",
      "\n",
      "loss:\t\t\t2.3419525741882117\n",
      "\n",
      "loss:\t\t\t2.340761775537769\n",
      "\n",
      "loss:\t\t\t2.3397427313801313\n",
      "\n",
      "loss:\t\t\t2.3387302999219255\n",
      "\n",
      "loss:\t\t\t2.3376933917045375\n",
      "\n",
      "loss:\t\t\t2.3373157332620544\n",
      "\n",
      "loss:\t\t\t2.336164647383977\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--35000\n",
      "loss:\t\t\t2.3350312669041964\n",
      "\n",
      "loss:\t\t\t2.334209007244055\n",
      "\n",
      "loss:\t\t\t2.3335110080614685\n",
      "\n",
      "loss:\t\t\t2.3315823779451055\n",
      "\n",
      "loss:\t\t\t2.3304141022690263\n",
      "\n",
      "loss:\t\t\t2.3298040259767197\n",
      "\n",
      "loss:\t\t\t2.3284263259412286\n",
      "\n",
      "loss:\t\t\t2.3275348975345187\n",
      "\n",
      "loss:\t\t\t2.326524647155223\n",
      "\n",
      "loss:\t\t\t2.3259423865422906\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--37500\n",
      "loss:\t\t\t2.325051344026625\n",
      "\n",
      "loss:\t\t\t2.3241396552796987\n",
      "\n",
      "loss:\t\t\t2.3229691268612482\n",
      "\n",
      "loss:\t\t\t2.3216346114195057\n",
      "\n",
      "loss:\t\t\t2.320728406150523\n",
      "\n",
      "loss:\t\t\t2.3195730482669608\n",
      "\n",
      "loss:\t\t\t2.3180720951319316\n",
      "\n",
      "loss:\t\t\t2.3171190978283907\n",
      "\n",
      "loss:\t\t\t2.316374251808075\n",
      "\n",
      "loss:\t\t\t2.3157011209249307\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--40000\n",
      "loss:\t\t\t2.3147043758924117\n",
      "\n",
      "loss:\t\t\t2.313634493956192\n",
      "\n",
      "loss:\t\t\t2.3126665413433938\n",
      "\n",
      "loss:\t\t\t2.3117370400987336\n",
      "\n",
      "loss:\t\t\t2.311119698893279\n",
      "\n",
      "loss:\t\t\t2.3101613521123476\n",
      "\n",
      "loss:\t\t\t2.309019687293793\n",
      "\n",
      "loss:\t\t\t2.3080733390499555\n",
      "\n",
      "loss:\t\t\t2.3071515075652194\n",
      "\n",
      "loss:\t\t\t2.306354175772452\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--42500\n",
      "loss:\t\t\t2.3051492823673523\n",
      "\n",
      "loss:\t\t\t2.304130498987454\n",
      "\n",
      "loss:\t\t\t2.3034213565805453\n",
      "\n",
      "loss:\t\t\t2.302518673080499\n",
      "\n",
      "loss:\t\t\t2.301268884176752\n",
      "\n",
      "loss:\t\t\t2.3007424328588586\n",
      "\n",
      "loss:\t\t\t2.299662640577267\n",
      "\n",
      "loss:\t\t\t2.2987287011405\n",
      "\n",
      "loss:\t\t\t2.298572191312993\n",
      "\n",
      "loss:\t\t\t2.2979259497715607\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--45000\n",
      "loss:\t\t\t2.2963963396743767\n",
      "\n",
      "loss:\t\t\t2.295873081412352\n",
      "\n",
      "loss:\t\t\t2.2950356475994975\n",
      "\n",
      "loss:\t\t\t2.293926319710137\n",
      "\n",
      "loss:\t\t\t2.292668380781603\n",
      "\n",
      "loss:\t\t\t2.2917734099413094\n",
      "\n",
      "loss:\t\t\t2.2905878992381115\n",
      "\n",
      "loss:\t\t\t2.289486180005147\n",
      "\n",
      "loss:\t\t\t2.2888572325673984\n",
      "\n",
      "loss:\t\t\t2.288575487495651\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--47500\n",
      "loss:\t\t\t2.2878072207594387\n",
      "\n",
      "loss:\t\t\t2.2872604535043397\n",
      "\n",
      "loss:\t\t\t2.2861459082798876\n",
      "\n",
      "loss:\t\t\t2.285405720241252\n",
      "\n",
      "loss:\t\t\t2.2842078489052113\n",
      "\n",
      "loss:\t\t\t2.283574741342664\n",
      "\n",
      "loss:\t\t\t2.2827611616803525\n",
      "\n",
      "loss:\t\t\t2.282075758565728\n",
      "\n",
      "loss:\t\t\t2.2811494951599625\n",
      "\n",
      "loss:\t\t\t2.2802459226303813\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--50000\n",
      "loss:\t\t\t2.2790267212917654\n",
      "\n",
      "loss:\t\t\t2.278272309615467\n",
      "\n",
      "loss:\t\t\t2.277374003013688\n",
      "\n",
      "loss:\t\t\t2.2767254265223085\n",
      "\n",
      "loss:\t\t\t2.276096123331829\n",
      "\n",
      "loss:\t\t\t2.2750879933309993\n",
      "\n",
      "loss:\t\t\t2.274279466875476\n",
      "\n",
      "loss:\t\t\t2.273560408720576\n",
      "\n",
      "loss:\t\t\t2.273178335812158\n",
      "\n",
      "loss:\t\t\t2.272516043698317\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--52500\n",
      "loss:\t\t\t2.2716658021247813\n",
      "\n",
      "loss:\t\t\t2.2709629911785303\n",
      "\n",
      "loss:\t\t\t2.2695972880897517\n",
      "\n",
      "loss:\t\t\t2.268871495424512\n",
      "\n",
      "loss:\t\t\t2.2680009811400113\n",
      "\n",
      "loss:\t\t\t2.267259883110121\n",
      "\n",
      "loss:\t\t\t2.266817704813279\n",
      "\n",
      "loss:\t\t\t2.266425043205443\n",
      "\n",
      "loss:\t\t\t2.2655794958280175\n",
      "\n",
      "loss:\t\t\t2.2648720792705475\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--55000\n",
      "loss:\t\t\t2.264169177975912\n",
      "\n",
      "loss:\t\t\t2.263378395383107\n",
      "\n",
      "loss:\t\t\t2.26248386890119\n",
      "\n",
      "loss:\t\t\t2.261470992684832\n",
      "\n",
      "loss:\t\t\t2.260899397765885\n",
      "\n",
      "loss:\t\t\t2.260245099594792\n",
      "\n",
      "loss:\t\t\t2.259383917001958\n",
      "\n",
      "loss:\t\t\t2.2591199264320903\n",
      "\n",
      "loss:\t\t\t2.258054685984722\n",
      "\n",
      "loss:\t\t\t2.2571271285947625\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--57500\n",
      "loss:\t\t\t2.256562330494173\n",
      "\n",
      "loss:\t\t\t2.2557693651196633\n",
      "\n",
      "loss:\t\t\t2.255294291492102\n",
      "\n",
      "loss:\t\t\t2.254479515967387\n",
      "\n",
      "loss:\t\t\t2.253940801478795\n",
      "\n",
      "loss:\t\t\t2.2531356924594084\n",
      "\n",
      "loss:\t\t\t2.2524642165206625\n",
      "\n",
      "loss:\t\t\t2.251878404525702\n",
      "\n",
      "loss:\t\t\t2.2513189978366013\n",
      "\n",
      "loss:\t\t\t2.2507129140735045\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--60000\n",
      "loss:\t\t\t2.249863792644627\n",
      "\n",
      "loss:\t\t\t2.2493261389857877\n",
      "\n",
      "loss:\t\t\t2.2486717415867634\n",
      "\n",
      "loss:\t\t\t2.2480434657369006\n",
      "\n",
      "loss:\t\t\t2.247070544055678\n",
      "\n",
      "loss:\t\t\t2.246784864127332\n",
      "\n",
      "loss:\t\t\t2.2464412523129487\n",
      "\n",
      "loss:\t\t\t2.245775254364861\n",
      "\n",
      "loss:\t\t\t2.2452675988303437\n",
      "\n",
      "loss:\t\t\t2.244607961352032\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--62500\n",
      "loss:\t\t\t2.2436708306333424\n",
      "\n",
      "loss:\t\t\t2.243068667300847\n",
      "\n",
      "loss:\t\t\t2.2424878700076945\n",
      "\n",
      "loss:\t\t\t2.241844899137442\n",
      "\n",
      "loss:\t\t\t2.241407234814634\n",
      "\n",
      "loss:\t\t\t2.2407448632781994\n",
      "\n",
      "loss:\t\t\t2.2399631456471396\n",
      "\n",
      "loss:\t\t\t2.2396844218168623\n",
      "\n",
      "loss:\t\t\t2.239324664503278\n",
      "\n",
      "loss:\t\t\t2.2387697766979575\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--65000\n",
      "loss:\t\t\t2.238417225254671\n",
      "\n",
      "loss:\t\t\t2.238043179283825\n",
      "\n",
      "loss:\t\t\t2.2372053204635516\n",
      "\n",
      "loss:\t\t\t2.236588511188617\n",
      "\n",
      "loss:\t\t\t2.236291161062986\n",
      "\n",
      "loss:\t\t\t2.235682539734694\n",
      "\n",
      "loss:\t\t\t2.235326238183991\n",
      "\n",
      "loss:\t\t\t2.2349598830165376\n",
      "\n",
      "loss:\t\t\t2.2344716670468117\n",
      "\n",
      "loss:\t\t\t2.2340466468559654\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--67500\n",
      "loss:\t\t\t2.2332899863997544\n",
      "\n",
      "loss:\t\t\t2.232849788151489\n",
      "\n",
      "loss:\t\t\t2.232030399088419\n",
      "\n",
      "loss:\t\t\t2.2315528211047253\n",
      "\n",
      "loss:\t\t\t2.2312004519026103\n",
      "\n",
      "loss:\t\t\t2.230752855058529\n",
      "\n",
      "loss:\t\t\t2.2304050900646204\n",
      "\n",
      "loss:\t\t\t2.2298926465872286\n",
      "\n",
      "loss:\t\t\t2.229355736244443\n",
      "\n",
      "loss:\t\t\t2.2287787184163554\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--70000\n",
      "loss:\t\t\t2.22833117139547\n",
      "\n",
      "loss:\t\t\t2.2277712598832795\n",
      "\n",
      "loss:\t\t\t2.2271079710355783\n",
      "\n",
      "loss:\t\t\t2.226627430971172\n",
      "\n",
      "loss:\t\t\t2.2260433980253276\n",
      "\n",
      "loss:\t\t\t2.2256254935307984\n",
      "\n",
      "loss:\t\t\t2.2251936066835807\n",
      "\n",
      "loss:\t\t\t2.224466815235463\n",
      "\n",
      "loss:\t\t\t2.223896805339131\n",
      "\n",
      "loss:\t\t\t2.223071081281688\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--72500\n",
      "loss:\t\t\t2.2225582948113822\n",
      "\n",
      "loss:\t\t\t2.22219079429879\n",
      "\n",
      "loss:\t\t\t2.221619761926313\n",
      "\n",
      "loss:\t\t\t2.221084585960052\n",
      "\n",
      "loss:\t\t\t2.220744651025706\n",
      "\n",
      "loss:\t\t\t2.22018109594235\n",
      "\n",
      "loss:\t\t\t2.2196198207528104\n",
      "\n",
      "loss:\t\t\t2.2191830640135692\n",
      "\n",
      "loss:\t\t\t2.2185819831638027\n",
      "\n",
      "loss:\t\t\t2.217958787664472\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--75000\n",
      "loss:\t\t\t2.21754689009582\n",
      "\n",
      "loss:\t\t\t2.217088863263049\n",
      "\n",
      "loss:\t\t\t2.21627206031613\n",
      "\n",
      "loss:\t\t\t2.2156496402422587\n",
      "\n",
      "loss:\t\t\t2.215222758222175\n",
      "\n",
      "loss:\t\t\t2.214790049533766\n",
      "\n",
      "loss:\t\t\t2.2142289681789142\n",
      "\n",
      "loss:\t\t\t2.2137982002989864\n",
      "\n",
      "loss:\t\t\t2.2133936175645945\n",
      "\n",
      "loss:\t\t\t2.212762405041352\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--77500\n",
      "loss:\t\t\t2.2122250056034134\n",
      "\n",
      "loss:\t\t\t2.2114244954503617\n",
      "\n",
      "loss:\t\t\t2.2107853280835044\n",
      "\n",
      "loss:\t\t\t2.210120637026267\n",
      "\n",
      "loss:\t\t\t2.2096477020925778\n",
      "\n",
      "loss:\t\t\t2.209133814783702\n",
      "\n",
      "loss:\t\t\t2.208635295361469\n",
      "\n",
      "loss:\t\t\t2.208016462384902\n",
      "\n",
      "loss:\t\t\t2.207609485059977\n",
      "\n",
      "loss:\t\t\t2.207299653562446\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--80000\n",
      "loss:\t\t\t2.206624505069293\n",
      "\n",
      "loss:\t\t\t2.20613486458078\n",
      "\n",
      "loss:\t\t\t2.2058453549440973\n",
      "\n",
      "loss:\t\t\t2.205511733715933\n",
      "\n",
      "loss:\t\t\t2.205156534711704\n",
      "\n",
      "loss:\t\t\t2.20488513212369\n",
      "\n",
      "loss:\t\t\t2.2046283325368274\n",
      "\n",
      "loss:\t\t\t2.2040016565722063\n",
      "\n",
      "loss:\t\t\t2.2038188991693826\n",
      "\n",
      "loss:\t\t\t2.2033735861888775\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--82500\n",
      "loss:\t\t\t2.2028630100692763\n",
      "\n",
      "loss:\t\t\t2.202534874179032\n",
      "\n",
      "loss:\t\t\t2.201974081867969\n",
      "\n",
      "loss:\t\t\t2.2015267969511294\n",
      "\n",
      "loss:\t\t\t2.20117033690696\n",
      "\n",
      "loss:\t\t\t2.2007202046842718\n",
      "\n",
      "loss:\t\t\t2.2006004125135283\n",
      "\n",
      "loss:\t\t\t2.2001175278626848\n",
      "\n",
      "loss:\t\t\t2.1995529523929194\n",
      "\n",
      "loss:\t\t\t2.1988788512723634\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--85000\n",
      "loss:\t\t\t2.198376585476539\n",
      "\n",
      "loss:\t\t\t2.197812606237501\n",
      "\n",
      "loss:\t\t\t2.1972873964637345\n",
      "\n",
      "loss:\t\t\t2.1967395320904846\n",
      "\n",
      "loss:\t\t\t2.196444798525336\n",
      "\n",
      "loss:\t\t\t2.1960184734223547\n",
      "\n",
      "loss:\t\t\t2.195444021648582\n",
      "\n",
      "loss:\t\t\t2.1949404313366765\n",
      "\n",
      "loss:\t\t\t2.1944878392212694\n",
      "\n",
      "loss:\t\t\t2.1939260994219847\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--87500\n",
      "loss:\t\t\t2.193558776625565\n",
      "\n",
      "loss:\t\t\t2.1929535312599944\n",
      "\n",
      "loss:\t\t\t2.19221751450053\n",
      "\n",
      "loss:\t\t\t2.191632242969514\n",
      "\n",
      "loss:\t\t\t2.1910892231671486\n",
      "\n",
      "loss:\t\t\t2.190757351598102\n",
      "\n",
      "loss:\t\t\t2.1901674080479347\n",
      "\n",
      "loss:\t\t\t2.189714157017339\n",
      "\n",
      "loss:\t\t\t2.189174333063917\n",
      "\n",
      "loss:\t\t\t2.188806411355651\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--90000\n",
      "loss:\t\t\t2.1883288804918526\n",
      "\n",
      "loss:\t\t\t2.187924791097311\n",
      "\n",
      "loss:\t\t\t2.1875102975289464\n",
      "\n",
      "loss:\t\t\t2.1870770521003844\n",
      "\n",
      "loss:\t\t\t2.1866629698860285\n",
      "\n",
      "loss:\t\t\t2.1861794576168876\n",
      "\n",
      "loss:\t\t\t2.1858678505216305\n",
      "\n",
      "loss:\t\t\t2.1854934842558094\n",
      "\n",
      "loss:\t\t\t2.1848675329307015\n",
      "\n",
      "loss:\t\t\t2.184404355198145\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--92500\n",
      "loss:\t\t\t2.183948538525443\n",
      "\n",
      "loss:\t\t\t2.1834783422616777\n",
      "\n",
      "loss:\t\t\t2.1830540114587516\n",
      "\n",
      "loss:\t\t\t2.182627207035753\n",
      "\n",
      "loss:\t\t\t2.1820878366822387\n",
      "\n",
      "loss:\t\t\t2.181454881692648\n",
      "\n",
      "loss:\t\t\t2.181013683310611\n",
      "\n",
      "loss:\t\t\t2.1805142282172443\n",
      "\n",
      "loss:\t\t\t2.1801387637762955\n",
      "\n",
      "loss:\t\t\t2.179653942915805\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--95000\n",
      "loss:\t\t\t2.1792224740479336\n",
      "\n",
      "loss:\t\t\t2.1788783266039813\n",
      "\n",
      "loss:\t\t\t2.178525721757434\n",
      "\n",
      "loss:\t\t\t2.1781910049279585\n",
      "\n",
      "loss:\t\t\t2.1778477185200122\n",
      "\n",
      "loss:\t\t\t2.177596488679152\n",
      "\n",
      "loss:\t\t\t2.1772444250292575\n",
      "\n",
      "loss:\t\t\t2.1769019557015072\n",
      "\n",
      "loss:\t\t\t2.1766801848321693\n",
      "\n",
      "loss:\t\t\t2.176441525104956\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--97500\n",
      "loss:\t\t\t2.17614632687775\n",
      "\n",
      "loss:\t\t\t2.176067039656441\n",
      "\n",
      "loss:\t\t\t2.175668974511097\n",
      "\n",
      "loss:\t\t\t2.1752520763724665\n",
      "\n",
      "loss:\t\t\t2.174899147979121\n",
      "\n",
      "loss:\t\t\t2.174694497338654\n",
      "\n",
      "loss:\t\t\t2.17433836473502\n",
      "\n",
      "loss:\t\t\t2.1740168993605926\n",
      "\n",
      "loss:\t\t\t2.173675109683344\n",
      "\n",
      "loss:\t\t\t2.1733009360811315\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--100000\n",
      "loss:\t\t\t2.173045892296508\n",
      "\n",
      "loss:\t\t\t2.1728746731293915\n",
      "\n",
      "loss:\t\t\t2.1724617559270034\n",
      "\n",
      "loss:\t\t\t2.172127336013184\n",
      "\n",
      "loss:\t\t\t2.1716963138362706\n",
      "\n",
      "loss:\t\t\t2.1711897235491393\n",
      "\n",
      "loss:\t\t\t2.170804715085573\n",
      "\n",
      "loss:\t\t\t2.170574036216926\n",
      "\n",
      "loss:\t\t\t2.170244239675137\n",
      "\n",
      "loss:\t\t\t2.169594211401231\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--102500\n",
      "loss:\t\t\t2.169597666336923\n",
      "\n",
      "loss:\t\t\t2.1691857177134413\n",
      "\n",
      "loss:\t\t\t2.168884021100127\n",
      "\n",
      "loss:\t\t\t2.1684795686626406\n",
      "\n",
      "loss:\t\t\t2.168083597720173\n",
      "\n",
      "loss:\t\t\t2.1677044910677226\n",
      "\n",
      "loss:\t\t\t2.1673095364642974\n",
      "\n",
      "loss:\t\t\t2.1667787934522047\n",
      "\n",
      "loss:\t\t\t2.1662519116166745\n",
      "\n",
      "loss:\t\t\t2.1659948829789792\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--105000\n",
      "loss:\t\t\t2.1656147949798124\n",
      "\n",
      "loss:\t\t\t2.1652500824962617\n",
      "\n",
      "loss:\t\t\t2.1651393951737274\n",
      "\n",
      "loss:\t\t\t2.164685733058527\n",
      "\n",
      "loss:\t\t\t2.1645256490491263\n",
      "\n",
      "loss:\t\t\t2.1643560441368117\n",
      "\n",
      "loss:\t\t\t2.1640105511750765\n",
      "\n",
      "loss:\t\t\t2.1637928947542884\n",
      "\n",
      "loss:\t\t\t2.163360203415469\n",
      "\n",
      "loss:\t\t\t2.1629371436584592\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--107500\n",
      "loss:\t\t\t2.162612763219552\n",
      "\n",
      "loss:\t\t\t2.162193171491099\n",
      "\n",
      "loss:\t\t\t2.16185683406272\n",
      "\n",
      "loss:\t\t\t2.161380131085702\n",
      "\n",
      "loss:\t\t\t2.1611231182108073\n",
      "\n",
      "loss:\t\t\t2.16075167019124\n",
      "\n",
      "loss:\t\t\t2.1604219768580837\n",
      "\n",
      "loss:\t\t\t2.16013349743242\n",
      "\n",
      "loss:\t\t\t2.15986170550082\n",
      "\n",
      "loss:\t\t\t2.1594472509424683\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--110000\n",
      "loss:\t\t\t2.159116192309457\n",
      "\n",
      "loss:\t\t\t2.1587324056171573\n",
      "\n",
      "loss:\t\t\t2.1585443639011204\n",
      "\n",
      "loss:\t\t\t2.1581156404742607\n",
      "\n",
      "loss:\t\t\t2.157693057638333\n",
      "\n",
      "loss:\t\t\t2.1574314392995633\n",
      "\n",
      "loss:\t\t\t2.1570775433805824\n",
      "\n",
      "loss:\t\t\t2.1568967366282905\n",
      "\n",
      "loss:\t\t\t2.156663230164808\n",
      "\n",
      "loss:\t\t\t2.156288334105255\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--112500\n",
      "loss:\t\t\t2.1558706007120674\n",
      "\n",
      "loss:\t\t\t2.155522827556584\n",
      "\n",
      "loss:\t\t\t2.155305397695705\n",
      "\n",
      "loss:\t\t\t2.1550194578765836\n",
      "\n",
      "loss:\t\t\t2.1547513417688564\n",
      "\n",
      "loss:\t\t\t2.154392973398409\n",
      "\n",
      "loss:\t\t\t2.154148339453529\n",
      "\n",
      "loss:\t\t\t2.153924066241806\n",
      "\n",
      "loss:\t\t\t2.1535423372247657\n",
      "\n",
      "loss:\t\t\t2.153207034217409\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--115000\n",
      "loss:\t\t\t2.1530675886583395\n",
      "\n",
      "loss:\t\t\t2.1526951927527493\n",
      "\n",
      "loss:\t\t\t2.152462624089704\n",
      "\n",
      "loss:\t\t\t2.152175744775723\n",
      "\n",
      "loss:\t\t\t2.1520028635676116\n",
      "\n",
      "loss:\t\t\t2.151782196355539\n",
      "\n",
      "loss:\t\t\t2.1515234499258096\n",
      "\n",
      "loss:\t\t\t2.1512206031621717\n",
      "\n",
      "loss:\t\t\t2.150831983315034\n",
      "\n",
      "loss:\t\t\t2.1504146879320776\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--117500\n",
      "loss:\t\t\t2.1499353817327544\n",
      "\n",
      "loss:\t\t\t2.149727988036227\n",
      "\n",
      "loss:\t\t\t2.1493099391666575\n",
      "\n",
      "loss:\t\t\t2.1490021468001808\n",
      "\n",
      "loss:\t\t\t2.1488487067923328\n",
      "\n",
      "loss:\t\t\t2.148576030290911\n",
      "\n",
      "loss:\t\t\t2.1482590774707497\n",
      "\n",
      "loss:\t\t\t2.1479121607836\n",
      "\n",
      "loss:\t\t\t2.1477781243283443\n",
      "\n",
      "loss:\t\t\t2.147310777215128\n",
      "\n",
      "saving model checkpoint to:\tD:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/checkpoint--120000\n",
      "loss:\t\t\t2.1470025184123775\n",
      "\n",
      "loss:\t\t\t2.146709694463752\n",
      "\n",
      "loss:\t\t\t2.1465006206732133\n",
      "\n",
      "loss:\t\t\t2.14626061928263\n",
      "\n",
      "loss:\t\t\t2.145899808787764\n",
      "\n",
      "\n",
      "\n",
      "saving final model to:\t D:/Users/Beth/Documents/climate-change-emo-analysis/fine-tuning-outputs/\n",
      "global step=121042, average loss=2.145875835060465\n"
     ]
    }
   ],
   "source": [
    "global_step, train_loss = train(train_dataset, device=device)\n",
    "print(f\"global step={global_step}, average loss={train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate perplexity of fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning evaluation on dataset train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=121042.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 6.686666488647461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.6867)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(train_dataset, MODEL, TOKENIZER, TRAIN_PARAMS['batch_size'], 'train', TRAIN_PARAMS['mlm_probability'], device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertfine",
   "language": "python",
   "name": "bertfine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
